{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Linear and Logistic Regressions\n",
    "## CSCI 4622 - Fall 2021\n",
    "***\n",
    "**Name**: $<$insert name here$>$\n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **11.59 PM on Monday, November 1st**.\n",
    "Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc.\n",
    "Your solutions to analysis questions should be done in Markdown directly below the associated question.\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors,\n",
    "but **you must write all code and solutions on your own**, and list any people or sources consulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle, gzip\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Linear Regression (55 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that Ridge regression adds a regularization term to the least square using the L2 norm.\n",
    "The objective is then to minimize:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{N}\\sum_{i=1}^{N} ||y_i-w^T x_i-b||^2 + \\alpha||w||^2\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf w$ denotes the set of parameters for the linear regression model,\n",
    " $\\alpha$ is the trade-off regularization parameter, and $N$ the number of samples\n",
    "The intercept $b$ can also be included in $(x_i)_{i\\leq N}$ by appending a constant feature to the data.\n",
    "You will be using the following data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CloudData:\n",
    "    def __init__(self):\n",
    "        # Data from: https://archive.ics.uci.edu/ml/datasets/Cloud\n",
    "        data = np.fromfile(\"data/cloud.data\", sep = \" \").reshape((1024, 10))\n",
    "        y = data[:, 6]\n",
    "        X = np.delete(data, 6, axis = 1)\n",
    "        \n",
    "        self.train_x = X[:800]\n",
    "        self.train_y = y[:800]\n",
    "        \n",
    "        self.test_x = X[800:]\n",
    "        self.test_y = y[800:]\n",
    "        \n",
    "class ForestData:\n",
    "    def __init__(self):\n",
    "        # Data from: http://archive.ics.uci.edu/ml/datasets/Forest+Fires\n",
    "        data = pd.read_csv(\"data/forestfires.csv\")\n",
    "        data = data.sample(frac = 1).reset_index(drop = True).drop(columns = [\"month\", \"day\"])\n",
    "        data[\"area\"] = np.log(data[\"area\"] + 1)\n",
    "        X = data.drop(columns = \"area\").values\n",
    "        y = data[\"area\"].values\n",
    "        \n",
    "        self.train_x = X[:400]\n",
    "        self.train_y = y[:400]\n",
    "        \n",
    "        self.test_x = X[400:]\n",
    "        self.test_y = y[400:]\n",
    "cloud_data = CloudData()\n",
    "forest_data = ForestData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To implement Ridge Regression, We'll use the solver from sklearn module `Ridge`\n",
    "([read more](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)).\n",
    "- 1.1 [5 points] Complete the `fit` and `evaluate` methods following the docstring description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "class Ridge:\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha:  regularization parameter\n",
    "        \"\"\"\n",
    "        self.alpha = alpha # our regularization / penalty term for weights\n",
    "        self._model = linear_model.Ridge(alpha, fit_intercept=True)  # Using sklearn module Ridge\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the ridge model, train it using the provided data\n",
    "        Calculate the number of non-zero coefficients in the model weights.\n",
    "        X: training features (n_samples, n_features)\n",
    "        y: target values (n_samples)\n",
    "        \n",
    "        RETURN :\n",
    "            num_nonzero_coeff : number of non-zero coefficients in the model weights\n",
    "        \"\"\"\n",
    "        #Workspace 1.1.a\n",
    "        #BEGIN \n",
    "        #TODO: Fit the ridge model and return the number of non-zero coefficients in the model weights.\n",
    "        \n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        #END\n",
    "        return num_nonzero_coeff\n",
    "\n",
    "    def evaluate(self, test_x, test_y):\n",
    "        \"\"\"\n",
    "        Compute Mean square error (MSE) between the predicted values and the actual values  of the test data\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_x: test features\n",
    "        test_y: test target\n",
    "\n",
    "        Returns MSE\n",
    "        -------\n",
    "        \"\"\"\n",
    "        #Workspace 1.1.b\n",
    "        #BEGIN\n",
    "        #TODO: predict based on the test features and return the mean_squared_error        \n",
    "\n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        #END \n",
    "        return mean_squared_error\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_true, y_hat):\n",
    "        return np.mean((y_true-y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.2 [5 points] For each of the datasets, produce 2 plots using `Ridge` :\n",
    "  - The number non-zero coefficients versus $\\alpha$\n",
    "  - Mean Squared Error (MSE) on test set versus $\\alpha$\n",
    " \n",
    " Use $\\alpha = 1, 50, 100, 200, 1000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [1, 50, 100, 200, 1000]\n",
    "#BEGIN Workspace 1.2\n",
    "\n",
    "#code here\n",
    "raise NotImplementedError()\n",
    "\n",
    "#END Workspace 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll move to Lasso regression. Lasso uses $l_1$ norm in the regularization term and minimizes:\n",
    "\\begin{align}\n",
    "\\frac{1}{2N}\\sum_i ||y_i-w^t x_i -b||^2 + \\alpha||w||_1\n",
    "\\end{align}\n",
    "\n",
    "We'll be using the `Lasso`solver from sklearn ([more details](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the Lasso class in the same way as you completed the Ridge class. We'll use the solver from sklearn module `Lasso`\n",
    "([read more](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)).\n",
    "- 1.3 [5 points] Complete the `fit` and `evaluate` methods following the docstring description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Lasso:\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha:  regularization parameter\n",
    "        \"\"\"                \n",
    "        self.alpha = alpha\n",
    "        self._model = linear_model.Lasso(alpha=alpha, fit_intercept=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Lasso model, train it using the provided data\n",
    "        Calculate the number of non-zero coefficients in the model weights.\n",
    "        X: training features (n_samples, n_features)\n",
    "        y: target values (n_samples)\n",
    "        \n",
    "        RETURN :\n",
    "            num_nonzero_coeff : number of non-zero coefficients in the model weights\n",
    "        \"\"\"\n",
    "        #Workspace 1.3.a\n",
    "        #BEGIN \n",
    "        #TODO: Fit the Lasso model and return the number of non-zero coefficients in the model weights.\n",
    "        \n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        #END\n",
    "        return num_nonzero_coeff\n",
    "\n",
    "    def evaluate(self, test_x, test_y):\n",
    "        \"\"\"\n",
    "        Compute Mean square error (MSE) between the predicted values and the actual values  of the test data\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_x: test features\n",
    "        test_y: test target\n",
    "\n",
    "        Returns MSE\n",
    "        -------\n",
    "        \"\"\"\n",
    "        #Workspace 1.3.b\n",
    "        #BEGIN\n",
    "        #TODO: predict based on the test features and return the mean_squared_error        \n",
    "        \n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        #END \n",
    "        return mean_squared_error\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_true, y_hat):\n",
    "        return np.mean((y_true-y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.4 [5 points] For each of the datasets, produce 2 plots using `Lasso` :\n",
    "  - The number non-zero coefficients versus $\\alpha$\n",
    "  - Mean Squared Error (MSE) on test set versus $\\alpha$\n",
    " \n",
    " Use $\\alpha = 0.01, 0.05, 0.1, 0.2, 0.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "# Workspace 1.4\n",
    "#BEGIN\n",
    "\n",
    "#Code here\n",
    "raise NotImplementedError()\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 [5 points] Compare the two algorithms on each data set: \n",
    "- Compare the number of non-zero coordinates of Ridge vs Lasso and their MSE on each dataset.\n",
    "- Which type of regression is better for each dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Write-up answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Elastic Net try to combine both types of regularization to get the best of both worlds by minimizing:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{2N} \\sum_i ||y_i - w^Tx_i -b||^2_2 + \\alpha\\beta||w||_1 + \\frac{\\alpha}{2}(1 - \\beta)||w||^2_2\n",
    "\\end{align}\n",
    "\n",
    "where $\\beta\\in[0,1]$ is the $l_1$ ratio ($\\beta=1$ for Lasso and $\\beta=0$ for Ridge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Complete the Elastic class in the same way as you completed the Ridge and Lasso class. We'll use the solver from sklearn module `ElasticNet`\n",
    "([read more](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)).\n",
    "- 1.6 [5 points] Complete the `fit` and `evaluate` methods following the docstring description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Elastic:\n",
    "    def __init__(self, alpha, beta=0.5):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha:  First regularization parameter\n",
    "        beta:   Second regularization parameter (default set to 0.5)        \n",
    "        \"\"\"                      \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self._model = linear_model.ElasticNet(alpha=alpha, l1_ratio=beta, fit_intercept=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Elastic model, train it using the provided data\n",
    "        Calculate the number of non-zero coefficients in the model weights.\n",
    "        X: training features (n_samples, n_features)\n",
    "        y: target values (n_samples)\n",
    "        \n",
    "        RETURN :\n",
    "            num_nonzero_coeff : number of non-zero coefficients in the model weights\n",
    "        \"\"\"\n",
    "        #Workspace 1.6.a\n",
    "        #BEGIN \n",
    "        #TODO: Fit the Elastic model and return the number of non-zero coefficients in the model weights.\n",
    "        \n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        #END\n",
    "        return num_nonzero_coeff\n",
    "\n",
    "    def evaluate(self, test_x, test_y):\n",
    "        \"\"\"\n",
    "        Compute Mean square error (MSE) between the predicted values and the actual values  of the test data\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_x: test features\n",
    "        test_y: test target\n",
    "\n",
    "        Returns MSE\n",
    "        -------\n",
    "        \"\"\"\n",
    "        #Workspace 1.6.b\n",
    "        #BEGIN\n",
    "        #TODO: predict based on the test features and return the mean_squared_error        \n",
    "\n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        #END \n",
    "        return mean_squared_error\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_true, y_hat):\n",
    "        return np.mean((y_true-y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tests cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.7 [5 points] For each of the datasets, produce 2 plots using `Elastic` :\n",
    "  - The number non-zero coefficients versus $\\alpha$\n",
    "  - Mean Squared Error (MSE) on test set versus $\\alpha$\n",
    " \n",
    "Use $\\alpha = 0.01, 0.05, 0.1, 0.2, 0.3, 0,5 , 1$  and  $\\beta= 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "#BEGIN Workspace 1.7\n",
    "\n",
    "#Code here\n",
    "raise NotImplementedError()\n",
    "\n",
    "#END Workspace 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8 [5 points] What are the pros and cons of each of three types of regressions we have implemented?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Workspace 1.8\n",
    "- Write-up answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Logistic Regression for Binary Classification (45 points)\n",
    "***\n",
    "The second part of this assignment will be dealing with Logistic Regression. While the name \"regression\" suggests otherwise, Logistic Regression is actaully used for classification.\n",
    "It's called regression because the model learns the continuous likelihood of multiple/a certain outcome.\n",
    "\n",
    "Our dataset is a subset of the MNIST dataset, which is a higher resolution of the sklearn digits data seen in HW1. \n",
    "\n",
    "In this problem you'll implement a Logistic Regression classifier to take drawings of either an eight\n",
    "or a nine and output the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryMNIST:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data for images of 9 and 8 only\n",
    "    \"\"\"\n",
    "    def __init__(self, location='./data/mnist.pklz'):\n",
    "        # You shouldn't have to modify this class, but you can if you'd like\n",
    "        # Load the dataset\n",
    "        with gzip.open(location, 'rb') as f:\n",
    "            self.train_x, self.train_y, self.test_x, self.test_y = pickle.load(f)\n",
    "\n",
    "        train_indices = np.where(self.train_y > 7)\n",
    "        self.train_x, self.train_y = self.train_x[train_indices], self.train_y[train_indices]\n",
    "        self.train_y = self.train_y - 8\n",
    "\n",
    "        test_indices = np.where(self.test_y > 7)\n",
    "        self.test_x, self.test_y = self.test_x[test_indices], self.test_y[test_indices]\n",
    "        self.test_y = self.test_y - 8\n",
    "\n",
    "binary_mnist = BinaryMNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Logistic Regression is about minimizing the negative log likelihood objective is defined as:\n",
    "\\begin{align}\n",
    "\\textrm{NLL}(w) = -\\frac{1}{N}\\sum_{i=1}^N \\left[y_i \\log \\sigma(w^T{x_i}) + (1-y_i)\\log(1 - \\sigma(w^Tx_i))\\right]\n",
    "\\end{align}\n",
    "where $\\sigma$ is the *sigmoid function* seen in class.\n",
    "\n",
    "Note that we're excluding the intercept since we'll be adding a constant column to the $(x_i)_i$. \n",
    "We will call it the zero-th column and the intercept will be $w_0$.\n",
    "\n",
    "The gradient of $NLL$ w.r.t $w$,  $\\frac{\\partial \\textrm{NLL}}{\\partial w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\textrm{NLL}}{\\partial w} = \\frac{1}{N} \\sum_i \\left[\\sigma(w^Tx_i)-y_i)\\right]x_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 2.1 [5 points] Finish the `calculate_score` function to return the output of applying\n",
    "the dot product of the weights with the input parameter\n",
    "\n",
    "- 2.2 [5 points] Finish the `sigmoid` function to return the output of applying the sigmoid function to the calculated score\n",
    "\n",
    "- 2.3 [5 points] Finish the `compute_gradient` function to return the derivative of the cost w.r.t. the weights\n",
    "\n",
    "- 2.4 [5 points] Finish the `batch_update` function so that it performs batch gradient descent using the provided batch data and updates the weight vector correspondingly\n",
    "\n",
    "- 2.5 [5 points] Finish the `fit` function so that it iterates over the training epochs and returns the Recall score (in-built sklearn metrics) on the validation data at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, eta = 0.1):\n",
    "        \"\"\"\n",
    "        Create a Logistic regression classifier\n",
    "        :param eta: Learning rate (the default is a constant value)\n",
    "        \"\"\"\n",
    "        self.w = None      # our array of weights\n",
    "        self.eta = eta # our learning rate\n",
    "        self.X = None   # features matrix\n",
    "        self.y = None   # labels / outcomes array\n",
    "\n",
    "    def calculate_score(self, x):\n",
    "        \"\"\"\n",
    "        :param x: This can be a single training example or it could be n training examples\n",
    "        :return score: Calculate the score that you will plug into the logistic function\n",
    "        \"\"\"\n",
    "        #Workspace 2.1\n",
    "        # TODO: Compute the score to be fed to the sigmoid function\n",
    "        #BEGIN\n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "        #END\n",
    "\n",
    "    def sigmoid(self, score, threshold=25.0):\n",
    "        \"\"\"\n",
    "        :param score: Either a real valued number or a vector to convert into a number between 0 and 1\n",
    "        :param threshold : Capping activations at 25 prevent overflow of np.exp() function.\n",
    "        :return sigmoid: Calculate the output of applying the sigmoid function to the score. This could be a single\n",
    "        value or a vector depending on the input.\n",
    "        \"\"\"\n",
    "        #BEGIN Workspace 2.2\n",
    "        # TODO: Complete this function to return the output of applying the sigmoid function to the score\n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "        #END Workspace 2.2\n",
    "    \n",
    "    def compute_gradient(self, x, h, y):\n",
    "        \"\"\"\n",
    "        :param x: Feature vector\n",
    "        :param h: the sigmoid of the scores computed from x\n",
    "        :param y: real class label\n",
    "        :return gradient: Return the derivative of the cost w.r.t to the weights\n",
    "        \"\"\"\n",
    "\n",
    "        #Workspace 2.3\n",
    "        # TODO: Finish this function to compute the gradient\n",
    "        #BEGIN\n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "        #END\n",
    "        \n",
    "    def batch_update(self, batch_x, batch_y):\n",
    "        \"\"\"\n",
    "        Single self.w update using the batch. We should average the gradient over the batch size\n",
    "        :param batch_x: NumPy array of features (size : size of batch X features + 1 for the intercept)\n",
    "        :param batch_y: Numpy array of class labels (size : size of batch )\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "\n",
    "        #Workspace 2.4\n",
    "        #BEGIN\n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "        #END        \n",
    "        \n",
    "    def fit(self, X, y, epochs=1, batch_size=1, validation_X=None, validation_y=None):\n",
    "        \"\"\"\n",
    "        :param X: training features\n",
    "        :param y: training labels\n",
    "        :param epochs: number of epochs\n",
    "        :param batch_size: size of batch for gradient update\n",
    "        :param validation_X: validation rows, should default to training data if not provided\n",
    "        :param validation_y: validation labels\n",
    "        :return: metric value at the end of each epoch on validation data\n",
    "        \"\"\"        \n",
    "        if validation_X is None:\n",
    "            validation_X, validation_y = X, y\n",
    "        metrics = []\n",
    "        \n",
    "        # Workspace  2.5\n",
    "        # TODO: Process x to append the zero-th constant column\n",
    "        # TODO: Compute average recall on the validation data at the end of each epoch\n",
    "        # HINT: Don't forget to initialize your weights!\n",
    "        \n",
    "        #BEGIN\n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "        #END\n",
    "        \n",
    "        return np.array(metrics)\n",
    "    \n",
    "    \n",
    "    def predict(self, test_x):\n",
    "        \"\"\"\n",
    "        :param test_x: n rows to predict on\n",
    "        :return: n predicted labels\n",
    "        \"\"\"\n",
    "        X = np.concatenate([np.ones((test_x.shape[0],1)), test_x], axis=1)\n",
    "        return np.round(self.sigmoid(self.calculate_score(X))).astype(int)\n",
    "\n",
    "\n",
    "    def optimize(self, batch_size):\n",
    "        \"\"\"\n",
    "        Shuffle the training data (use permutation) and send batches to batch_update to update $self.w$\n",
    "        Parameters\n",
    "        :batch_size: size of batches to use for stochastic gradient\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "\n",
    "        indices = np.random.permutation(len(self.X))\n",
    "        for i in range(0, self.X.shape[0], batch_size):\n",
    "            batch_x = self.X[indices[i:i+batch_size]]\n",
    "            batch_y = self.y[indices[i:i+batch_size]]\n",
    "            self.batch_update(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tests cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "After completing the class above, loop over the training data and perform batch training with `batch_size = 1` for `10 epochs`, and five different values of `eta`, i.e, [$.0001, .01, .1, .5, 1$]. Train your model and do the following:\n",
    "\n",
    "2.6 [5 points] Create a new classifier object and Using the `fit` method, plot the recall trend for the different values of eta on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = [.0001,.001,.01, 1, 5]\n",
    "np.random.seed(42)\n",
    "for eta in etas:\n",
    "#Workspace 2.6\n",
    "#BEGIN\n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.7 [5 points] Create a new classifier object and Using the `fit` method, plot the values of recall versus eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = [.0001,.001,.01, 1, 5]\n",
    "#Workspace 2.7\n",
    "#BEGIN\n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we want to analyze the effect of varying the batch size. We fix `eta = 0.1` and `epochs = 10` and we want to examine the recall on the test set at the end of the training for `batch_size` in [$1, 2, 4, 8, 12, 16$].\n",
    "\n",
    "- 2.8 [5 points] Produce a plot of the recall at the end of the training as a function of the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes= [1, 2, 4, 8, 12, 16]\n",
    "recalls = []\n",
    "#Workspace 2.8\n",
    "#BEGIN\n",
    "        #Code here\n",
    "        raise NotImplementedError()\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9 [5 points] How does the learning rate (`eta`) and the number of epochs affect the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace 2.9\n",
    "- Write-up answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.BONUS [10 Points] Since we're done with the binary regression, we will try to add Ridge regularization:\n",
    "\\begin{align}\n",
    "\\textrm{NLL}(w) = -\\frac{1}{N}\\sum_{i=1}^N \\left[y_i \\log \\sigma(w^T{x_i}) + (1-y_i)\\log(1 - \\sigma(w^Tx_i))\\right] + \\alpha {||w||^2}\n",
    "\\end{align}\n",
    "\n",
    "We will need to add an `alpha` parameter in our `LogisticRegression` class (with a default value) \n",
    "\n",
    "First, write the gradient formula in the cell below. Then, edit your `compute_gradient` to account for the regularization term. Ensure the rest of your functions still run correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<2.BONUS Write your modified gradient formula here>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
