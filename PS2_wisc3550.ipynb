{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 - Naive Bayes, Decision Trees with ensemble methods\n",
    "## CSCI 4622 - Fall 2021\n",
    "***\n",
    "**Name**: Willem Scott\n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **1:50PM on October 6th**.\n",
    "\n",
    "Submit only this Jupyter notebook to Canvas. Do not compress it using tar, rar, zip, etc.\n",
    "Your solutions to analysis questions should be done in Markdown directly below the associated question.\n",
    "\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors, \n",
    "but **you must write all code and solutions on your own**, and list any people or sources consulted.\n",
    "The only exception to this rule is that you may copy code directly from your own solution to homework 1.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "Your task for this homework is to build a naive Bayes and a decision tree classifiers in the first 2 problems.\n",
    "The last problem is about ensemble methods using scikit-learn decision tree as a weak learner.\n",
    "We'll explore bagging, boosting (AdaBoost) and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from time import time\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Naive Bayes [25 points]\n",
    "***\n",
    "Consider the problem of predicting whether a person has a college degree based on age, salary, and Colorado residency.\n",
    "The dataset looks like the following.\n",
    "\n",
    "|Age|Salary|Colorado Residency| College degree|\n",
    "|:------:|:-----------:| :----------:|--:|\n",
    "| 27 | 41,000 | Yes | Yes |\n",
    "| 61 | 52,000 | No | No |\n",
    "| 23 | 24,000 | Yes | No |\n",
    "| 29 | 77,000 | Yes | Yes |\n",
    "| 32 | 48,000 | No | Yes |\n",
    "| 57 | 120,000 | Yes | Yes |\n",
    "| 22 | 38,000 | Yes | Yes |\n",
    "| 41 | 45,000 | Yes | No |\n",
    "| 53 | 26,000 | No | No |\n",
    "| 48 | 65,000 | Yes | Yes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = np.array([[27 , 41000 , 1],\n",
    "              [61 , 52000 , 0],\n",
    "              [23 , 24000 , 1],\n",
    "              [29 , 77000 , 1],\n",
    "              [32 , 48000 , 0],\n",
    "              [57 , 120000 , 1],\n",
    "              [22 , 38000 , 1],\n",
    "              [41 , 45000 , 1],\n",
    "              [53 , 26000 , 0],\n",
    "              [48 , 65000 , 1]])\n",
    "labels = np.array([1, 0, 0, 1, 1, 1, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1 What is our expected accuracy for the baseline case where we predict one label for all rows? (*2 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#BEGIN Workspace 1.1\n",
    "\n",
    "#TODO: accuracy of the baseline using no features\n",
    "\n",
    "60%\n",
    "\n",
    "#END Workspace 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we have to find a way to deal with the continuous features. For now, let's put them into binary bins based on threshold arguments to our classifier - so we can treat this as a tuning parameter.\n",
    "\n",
    "1.2 Complete `threshold_features` to convert age and salary features to binary ones using the threshold arguments. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def threshold_features(features, age_threshold, salary_threshold):\n",
    "    binary_X = features * 1 #This row just creates a \"hard copy\" of the X array so we can manipulate it as needed\n",
    "\n",
    "    #BEGIN Workspace 1.2\n",
    "    #TODO: Threshold the corresponding features\n",
    "    binary_X[:, 0] = binary_X[:, 0] >= age_threshold\n",
    "    binary_X[:, 1] = binary_X[:, 1] >= salary_threshold \n",
    "\n",
    "    #END Workspace 1.2\n",
    "\n",
    "    return binary_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As seen during the class, given a row $(x_1, x_2, x_3)$, the naive Bayes classifier should assign the label $y$ that\n",
    "maximizes:\n",
    "\n",
    "\\begin{align}\n",
    "\\log [p(y) \\prod_i p(x_i | y)] = \\log p(y) + \\sum_{i} \\log p(x_i | y)\n",
    "\\end{align}\n",
    "\n",
    "$p(y)$ and $p(x_i | y)$ are computed using the training set (during `fit` call).\n",
    "\n",
    "We have defined $p(x_i | y)$ as :\n",
    "\\begin{align}\n",
    "p(x_i | y) = \\frac{N_{y,i}}{N_y}\n",
    "\\end{align}\n",
    "where $N_{y,i}$ is the number of rows where $y$ and $x_i$ occur together, and $N_y = \\sum_i N_{y,i}$.\n",
    "\n",
    "1.3 Complete the `fit` call by computing the counts and joint counts. Hint: Use `features_counts` to store the contingency\n",
    "table $N_{y,i}$ for each feature $i$ and then use them to compute $\\log p(x_i | y)$ (*10 points*)\n",
    "\n",
    "1.4 Complete the `predict` call (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    \"\"\"\n",
    "    NaiveBayes classifier for binary features and binary labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes_counts = None\n",
    "        self.features_counts = []\n",
    "        self.classes_log_probabilities = None\n",
    "        self.features_log_probabilities = [] # same structure as features_count\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: binary np.array of shape (n_samples, n_features)\n",
    "        y: corresponding labels of shape (n_samples,)\n",
    "        Returns\n",
    "        -------\n",
    "        Trained classifier\n",
    "        \"\"\"\n",
    "\n",
    "        #BEGIN Workspace 1.3\n",
    "        #TODO: Compute the counts and joint counts\n",
    "        \n",
    "        #P(A | B) = P(B | A)P(A)/P(B)\n",
    "        # P of y given X_i = P(X_i | y)P(y)/P(X_i) for X_1 = 0 or 1\n",
    "        self.classes_counts = np.array([len(y) - y.sum(), y.sum()])\n",
    "        self.classes_log_probabilities = np.log(self.classes_counts/len(y))\n",
    "        \n",
    "        print(self.classes_counts)\n",
    "        self.features_counts = np.zeros((X.shape[1], 2))\n",
    "        self.features_log_probabilities = np.zeros((X.shape[1], 2))\n",
    "        for i in range(X.shape[1]):\n",
    "            self.features_counts[i, :] = ((X[:, i] == y[i]).sum(), (X[:, i] != y[i]).sum())\n",
    "            self.features_log_probabilities[i, :] = np.log(\n",
    "                (\n",
    "                    (X[:, i] == y[i]).sum()/self.classes_counts[0],\n",
    "                    (X[:, i] != y[i]).sum()/self.classes_counts[1]\n",
    "                )\n",
    "            )\n",
    "        \n",
    "                \n",
    "        #END Workspace 1.3\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        joint_log_likelihood = np.zeros((x_test.shape[0], self.classes_counts.shape[0]))\n",
    "        y_hat = []\n",
    "        #BEGIN Workspace 1.4\n",
    "        #TODO: Find the corresponding labels using Naive bayes logic\n",
    "        \n",
    "        #END Workspace 1.4\n",
    "        \n",
    "        # logùëù(ùë¶)+‚àëùëñlogùëù(ùë•ùëñ|ùë¶)\n",
    "        \n",
    "        for row in x_test:\n",
    "            y_hat.append(\n",
    "                max(\n",
    "                    range(2),\n",
    "                    key=lambda label: self.classes_log_probabilities[label] +\n",
    "                    sum(\n",
    "                        map(lambda a: a[0][a[1]], \n",
    "                            zip(self.features_log_probabilities, row))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "                \n",
    "                \n",
    "        \n",
    "        return np.array(y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.5 Using age 30 and salary 40,000 as thresholds, transform the features and evaluate (accuracy) the NaiveBayes classifier\n",
    "on the training data. (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6]\n",
      "Prediction: [1 1 1 1 1 1 1 1 1 1]\n",
      "Actual: [1 0 0 1 1 1 1 0 0 1]\n",
      "Accuracy: 6\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayes()\n",
    "#BEGIN Workspace 1.5\n",
    "#TODO: Transform features to binary features, fit the classifier, report the accuracy\n",
    "clf.fit(threshold_features(features, 30, 40000), labels)\n",
    "pred = clf.predict(threshold_features(features, 30, 40000))\n",
    "print(\"Prediction:\", pred)\n",
    "print(\"Actual:\", labels)\n",
    "print(\"Accuracy:\", (pred == labels).sum())\n",
    "\n",
    "#END Workspace 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Bonus question** 1.6 Use the attribute `alpha` of the NaiveBayes to convert it to the smoothed NaiveBayes presented during the class. (*5 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 2 - Decision trees [25 points]\n",
    "***\n",
    "The goal of this problem is to implement the core elements of the Decision Tree classifier.\n",
    "We do not expect a highly efficient implementation of the functions since the ensemble methods will use the implementation\n",
    "from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by considering the variable *Colorado residency*.\n",
    "\n",
    "The leaf nodes of a decision tree act in the same way as in question (1.1) where no feature is used.\n",
    "\n",
    "2.1 Complete `get_error_in_leaf` to return the count of misclassified instances. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_error_in_leaf(y, indices):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param indices: the subset of indexes in the leaf node\n",
    "    :return: Returns the number of errors in a leaf node of a decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    error_count = 0\n",
    "    #BEGIN Workspace 2.1\n",
    "    #TODO: Compute the number of errors in the leaf node (no feature is used)\n",
    "    \n",
    "    #END Workspace 2.1\n",
    "    return error_count\n",
    "\n",
    "\n",
    "def value_split_binary_feature(x, y, feature_index, root, criteria_func):\n",
    "    \"\"\"Will be used later to evaluate the criteria gain\"\"\"\n",
    "    left_child = [i for i in root if x[i, feature_index] == 0]\n",
    "    right_child = [i for i in root if x[i, feature_index] == 1]\n",
    "    return criteria_func(y, root, left_child, right_child)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use information gain criteria to decide how to split the root node of our decision tree.\n",
    "\n",
    "2.2 Complete the `entropy` function. (*5 points*)\n",
    "\n",
    "2.3 Complete the `information_gain_criteria` to compute the information gained by splitting the root node.\n",
    " Print the gain value for splitting based on *Colorado residency* (*5 points*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(y, indices):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param indices: the indices of data points\n",
    "    :return: Returns the entropy in the labels for the data points in indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    entropy_value = 0\n",
    "    if len(indices) == 0: # deal with corner case when there is no data point.\n",
    "        return entropy_value\n",
    "    else:\n",
    "        #BEGIN Workspace 2.2\n",
    "        #TODO: Compute the entropy of the labels from indices\n",
    "        pass\n",
    "        #END Workspace 2.2\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain_criteria(y, root, left_child, right_child):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param root: indices of all the data points in the root\n",
    "    :param left_child: the subset of indices in the left child\n",
    "    :param right_child: the subset of indices in the right child\n",
    "    :return: information gain of the split\n",
    "    \"\"\"\n",
    "    information_gain = 0\n",
    "    #BEGIN Workspace 2.3.a\n",
    "    #TODO: Compute the information gain of the split\n",
    "    \n",
    "    #END Workspace 2.3.a\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_id = 2\n",
    "info_gain = 0\n",
    "#BEGIN Workspace 2.3.b\n",
    "#TODO: report the information gain of the split based on Colorado Residency\n",
    "\n",
    "#END Workspace 2.3.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to deal with continuous features for the decision tree.\n",
    "One way to deal with continuous (or ordinal) data is to define binary features based on thresholding as we've done\n",
    "for NaiveBayes. But we have to find the optimal threshold based on the criteria we're using.\n",
    "\n",
    "2.4 Complete the `value_split_continuous_feature` by trying different possible threshold values of feature\n",
    "of index `feature_index` and return the best criteria value and threshold. (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def value_split_continuous_feature(x, y, feature_index, root, criteria_func=information_gain_criteria):\n",
    "    \"\"\"\n",
    "    :param x: all feature values\n",
    "    :param y: all labels\n",
    "    :param feature_index: feature id to split the tree based on\n",
    "    :param root: indexes of all the data points in the root\n",
    "    :param criteria_func: the splitting criteria function\n",
    "    :return: Return the best value and its corresponding threshold by splitting based on a continuous feature.\n",
    "    \"\"\"\n",
    "\n",
    "    best_value, best_thres = 0, 0\n",
    "\n",
    "    #BEGIN Workspace 2.4\n",
    "    #TODO: Complete the function as detailed in the question and function description\n",
    "\n",
    "    #END Workspace 2.4\n",
    "\n",
    "    return best_value, best_thres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.5 Find the best thresholds for age and salary. Print their corresponding information gains. (*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "root = list(range(len(labels))) # root includes all data points\n",
    "#BEGIN Workspace 2.5\n",
    "#TODO: Report the best thresholds for age and salary and their split information gains\n",
    "\n",
    "#END Workspace 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Based on the obtained information gains, if we build a decision stump (decision tree with depth 1) greedily,\n",
    "which feature should we choose? Why? What's the resulting accuracy? (*2 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#BEGIN Workspace 2.6.a\n",
    "\n",
    "#TODO: Which feature should we pick for the decision stump? Why?\n",
    "\n",
    "#END Workspace 2.6.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#BEGIN Workspace 2.6.b\n",
    "#TODO: Split based on the chosen feature and compute the accuracy (use get_error_in_leaf)\n",
    "\n",
    "#BEGIN Workspace 2.6.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question**\n",
    "\n",
    "2.7 You now have all the ingredients to build a decision tree recursively.\n",
    "You can build a decision tree of depth two and report its classification error on the training data and the tree.(*5 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#BEGIN Workspace 2.7\n",
    "#TODO: Build a Decision Tree of Depth 2 using age, salary and the previously computed thresholds\n",
    "\n",
    "\n",
    "#END Workspace 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3  - Decision Tree Ensembles: Bagging and (BONUS: Boosting) [50 points]\n",
    "---\n",
    "\n",
    "We are going to predict house price levels using decision tree ensembles.\n",
    "\n",
    "In this classification problem, we compare Decision trees and it's ensembles - Bagging and Boosting on House Price prediction [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "Our *weak learner* for this problem is the DecisionTreeClassifier from scikit-learn with `max_depth=10`.\n",
    "\n",
    "We start first by loading preprocessed data that we'll use. Since the original data is for regression, we have first to transform\n",
    "`y_train` and `y_test` to discrete values reflecting price level.\n",
    "\n",
    "|Price range| Label|\n",
    "|:----------:|--:|\n",
    "| $ P < $125000|0|\n",
    "|125000$\\leq P < $ 160000| 1 |\n",
    "|160000$ \\leq P < $ 200000| 2 |\n",
    "|200000$ \\leq P $ | 3 |\n",
    "\n",
    "3.1 Start by transforming`y_train` and `y_test` to discrete values using the provided ranges. (*3 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] (1166, 79)\n",
      "[0 1 2 3] (292, 79)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/test_train.pkl','rb'))\n",
    "#BEGIN Workspace 3.1\n",
    "#TODO: Discretize y_train and y_test\n",
    "\n",
    "y_train = np.array(list(map(lambda p: 0 if p < 125000 else (1 if p < 160000 else (2 if p < 200000 else 3)), y_train)))\n",
    "y_test = np.array(list(map(lambda p: 0 if p < 125000 else (1 if p < 160000 else (2 if p < 200000 else 3)), y_test)))\n",
    "\n",
    "#END Workspace 3.1\n",
    "print(np.unique(y_train), X_train.shape)\n",
    "print(np.unique(y_test), X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.2 Complete the `ensemble_test` class to `fit` the model received as parameter and store the metrics and running time. (*5 points*)\n",
    "\n",
    "3.3 Complete `plot_metric` to show and compare different statistics of each model in a bar chart. (*5 points*)\n",
    "\n",
    "Later we will also use `ensemble_test` class to plot score, metric and time taken to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_weak_learner():\n",
    "    \"\"\"Return a new instance of out chosen weak learner\"\"\"\n",
    "    return DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "class EnsembleTest:\n",
    "    \"\"\"\n",
    "        Test multiple model performance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_train, y_train, x_test, y_test):\n",
    "        \"\"\"\n",
    "        initialize data partitions\n",
    "        \"\"\"\n",
    "        self.scores = {}\n",
    "        self.execution_time = {}\n",
    "        self.metric = {}\n",
    "        self.x_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.score_name ='Mean accuracy'\n",
    "        self.metric_name = 'Precision(micro)'\n",
    "\n",
    "    def fit_model(self, model, name):\n",
    "        \"\"\"\n",
    "        Fit the model on train data.\n",
    "        predict on test and store score and execution time for each fit.\n",
    "        :param model: model\n",
    "        :param name: name of model\n",
    "        \"\"\"\n",
    "        start = time()\n",
    "        #BEGIN Workspace 3.2\n",
    "        #TODO: Fit the model and get the predictions\n",
    "        #       train and test data are treated as global variables\n",
    "\n",
    "        #Hint: self.scores[name] = precision_score(?, average=\"micro\") # in multi-class, micro implies treating it as binary precision\n",
    "        #Hint self.metric[name] = Accuracy\n",
    "        \n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        pred = model.predict(self.x_test)\n",
    "        self.scores[name] = precision_score(self.y_test, pred, average=\"micro\")\n",
    "        self.metric[name] = np.array([x.mean() for x in (pred == self.y_test)]).mean()\n",
    "        \n",
    "        #END Workspace 3.2\n",
    "        self.execution_time[name] = time() - start\n",
    "\n",
    "    def print_result(self):\n",
    "        \"\"\"\n",
    "            print results for all models trained and tested.\n",
    "        \"\"\"\n",
    "        models_cross = pd.DataFrame({\n",
    "            'Model'         : list(self.metric.keys()),\n",
    "             self.score_name     : list(self.scores.values()),\n",
    "             self.metric_name    : list(self.metric.values()),\n",
    "            'Execution time': list(self.execution_time.values())})\n",
    "        print(models_cross.sort_values(by=self.score_name, ascending=False))\n",
    "\n",
    "    def plot_metric(self):\n",
    "        #BEGIN Workspace 3.3\n",
    "        #TODO: plot bar chart for each metric : time, metric, score\n",
    "        plt.bar(self.metric.keys(), self.metric.values())\n",
    "        plt.bar(self.scores.keys(), self.scores.values())\n",
    "        plt.bar(self.execution_time.keys(), self.execution_time.values())\n",
    "        #END Workspace 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.4 Test `EnsembleTest` using our weak learner returned by `get_weak_learner` (*2 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Model, Mean accuracy, Precision(micro), Execution time]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(X_train,y_train,X_test,y_test)\n",
    "#BEGIN Workspace 3.4\n",
    "#TODO: Initialize weak learner and fit ensemble_handler\n",
    "\n",
    "\n",
    "#END Workspace 3.4\n",
    "ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging:**\n",
    "\n",
    "Bagging consists of training a set of weak learners using random subsets of the train data.\n",
    "\n",
    "3.5 First, complete `sample_data` to return a random sample of size `sample_ratio * len(X_train)` of features and labels (*2 points*)\n",
    "\n",
    "3.6 Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on random sample of the data (*5 points*)\n",
    "\n",
    "3.7 Complete `predict` method to return the most likely label by combining different estimators predictions. \n",
    "Use a simple majority / plurality vote system similar to the one used in your KNNClassifier in Problem Set 1. However, in this case, to break a tie you should use `predict_log_proba` or `predict_proba` method of DecisionTreeClassifier:\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba) (*2 points*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaggingEnsemble(object):\n",
    "\n",
    "    def __init__(self, n_estimators, sample_ratio):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.estimators = []\n",
    "\n",
    "    def sample_data(self, x_train, y_train):\n",
    "        x_sample, y_sample = None, None\n",
    "        #BEGIN Workspace 3.5\n",
    "        #TODO: sample random subset of size sample_ratio * len(X_train)\n",
    "        opts = np.random.choice(range(x_train.shape[0]), size=int(self.sample_ratio * x_train.shape[0]), replace=False)\n",
    "        x_sample = x_train[opts]\n",
    "        y_sample = y_train[opts]\n",
    "        \n",
    "        #END Workspace 3.5\n",
    "        return x_sample, y_sample\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        for _ in range(self.n_estimators):\n",
    "            #BEGIN Workspace 3.6\n",
    "            #TODO: sample data and create new weak learner trained on the sample\n",
    "            test_x, test_y = self.sample_data(x_train, y_train)\n",
    "            learner = get_weak_learner()\n",
    "            learner.fit(test_x, test_y)\n",
    "            self.estimators.append(learner)\n",
    "            \n",
    "            #END Workspace 3.6\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predicted_proba = None\n",
    "        answer = 0\n",
    "        #BEGIN Workspace 3.7\n",
    "        #TODO: go through the trained estimators and acummualte their predicted_proba to get the mostly likely label\n",
    "        for est in self.estimators:\n",
    "            if predicted_proba is None:\n",
    "                predicted_proba = est.predict(X_test)\n",
    "            else:\n",
    "                predicted_proba += est.predict(X_test)\n",
    "        #END Workspace 3.7\n",
    "        return (predicted_proba / self.n_estimators).round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model  Mean accuracy  Precision(micro)  Execution time\n",
      "0  Bagging       0.726027          0.726027        0.141042\n"
     ]
    }
   ],
   "source": [
    "# This cell should run without errors\n",
    "ensemble_handler.fit_model(BaggingEnsemble(10, 0.9),'Bagging')\n",
    "ensemble_handler.print_result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Random Forest**\n",
    "Random Forest has an additional layer of randomness compared to Bagging: we also sample a random subset of the features.\n",
    "\n",
    "3.8 First, complete `sample_data` to return a random sample of size `sample_ratio * len(X_train)` of labels and `feature_ratio * num_features` of features (*2 points*)\n",
    "\n",
    "3.9 Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on random sample of the data.\n",
    "Make sure to keep track of the sampled features for each estimator to use them in the prediction step. (*5 points*)\n",
    "\n",
    "3.10 Complete `predict` method to return the most likely label by combining different estimators predictions.\n",
    "Use a simple majority / plurality vote system similar to the one used in your KNNClassifier in Problem Set 1. However, in this case, to break a tie you should use `predict_log_proba` or `predict_proba`  method of DecisionTreeClassifier:\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba) (*3 points*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "\n",
    "    def __init__(self, n_estimators, sample_ratio, features_ratio):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.features_ratio = features_ratio\n",
    "        self.estimators = []\n",
    "        self.features_indices = []\n",
    "\n",
    "    def sample_data(self, x_train, y_train):\n",
    "        X_sample, y_sample, features_indices = None, None, None\n",
    "        #BEGIN Workspace 3.8\n",
    "        #TODO: sample random subset of size sample_ratio * len(X_train) and subset of features of size\n",
    "        #         features_ratio * num_features\n",
    "\n",
    "\n",
    "        opts = np.random.choice(range(len(x_train)), size=int(self.sample_ratio * x_train.shape[0]), replace=False)\n",
    "        features_indices = np.random.choice(range(x_train.shape[1]), size=int(self.features_ratio * x_train.shape[1]), replace=False)\n",
    "        \n",
    "        x_sample = x_train[opts][:, features_indices]\n",
    "        y_sample = y_train[opts]\n",
    "        \n",
    "        \n",
    "        #END Workspace 3.8\n",
    "        return x_sample, y_sample, features_indices\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        for _ in range(self.n_estimators):\n",
    "            #BEGIN Workspace 3.9\n",
    "            #TODO: sample data with random subset of rows and features using sample_data\n",
    "            #Hint: keep track of the features indices in features_indices to use in predict\n",
    "            (test_x, test_y, indices) = self.sample_data(x_train, y_train)\n",
    "            learner = get_weak_learner()\n",
    "            learner.fit(test_x, test_y)\n",
    "            self.estimators.append(learner)\n",
    "            self.features_indices.append(indices)\n",
    "            #END Workspace 3.9\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predicted_proba = 0\n",
    "        answer = 0\n",
    "        #BEGIN Workspace 3.10\n",
    "        #TODO: compute cumulative sum of predict proba from estimators and return the labels with highest likelihood\n",
    "        for ind, est in zip(self.features_indices, self.estimators):\n",
    "            if predicted_proba is None:\n",
    "                predicted_proba = est.predict(X_test[:, ind])\n",
    "            else:\n",
    "                predicted_proba += est.predict(X_test[:, ind])\n",
    "        #END Workspace 3.7\n",
    "        return (predicted_proba / self.n_estimators).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model  Mean accuracy  Precision(micro)  Execution time\n",
      "1  RandomForest       0.767123          0.767123        0.236771\n",
      "0       Bagging       0.726027          0.726027        0.141042\n"
     ]
    }
   ],
   "source": [
    "# This cell should run without errors\n",
    "ensemble_handler.fit_model(RandomForest(20, sample_ratio=0.9, features_ratio=0.8), 'RandomForest')\n",
    "ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting (BONUS 10 Points)**\n",
    "\n",
    "There are different methods of boosting, but we'll focus in this problem on Adaptive Boosting (AdaBoost).\n",
    "The logic of AdaBoost is to \"push\" the new learner to give more importance to previously misclassified data. We present\n",
    "below the multiclass variant of AdaBoost [SAMME](https://web.stanford.edu/~hastie/Papers/samme.pdf). We denote $K$ the number of classes.\n",
    "\n",
    "AdaBosst is performed by increasing the weights of misclassified simple after each iteration:\n",
    "- Start with equal weights $W_1 = (w_i), $ where   $w_i = \\frac{1}{\\texttt{n_samples}}$\n",
    "- at step j:\n",
    "    - Train estimator $h_j$ using weights $W_j$\n",
    "    - Find the weighted error rate $\\epsilon_j$ using $W_j$: $\\epsilon_j=\\frac{\\sum_i w_i \\delta(\\hat{y}_i, y_i)}{\\sum_i w_i}$\n",
    "    - Choose $\\alpha_j = \\log \\frac{1-\\epsilon_j}{\\epsilon_j} + \\log(K-1)$\n",
    "    - Update $W_j$ using: $w_i \\leftarrow w_i \\exp(\\alpha_j \\delta(\\hat{y_i}, y_i)) $\n",
    "    - Normalize $W_j$ to have sum 1\n",
    "- Global estimator is $H = \\sum_j \\alpha_j h_j$\n",
    "\n",
    "$\\hat{y}$ are the predicted labels, and $\\delta$ the Kronecker function, equal to 1 when the two argument are equal, 0 otherwise.\n",
    "\n",
    "\n",
    "Bonus.1 Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on the same data but with different\n",
    "samples weights as detailed in the algorithm. Keep track of $(\\alpha_i)$\n",
    "\n",
    "Bonus.2 Complete `predict` method to return the predicted label using the global estimator $H$. Hint:\n",
    "use one hot encoding of the predicted labels from the weak learners and cumulate the prediction with weights $\\alpha_j$ \n",
    "\n",
    "Notice that if the weak learner is consistent (0 error rate on the training set), AdaBoost $\\alpha_j$ are no longer defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AdaBoost(object):\n",
    "\n",
    "    def __init__(self, n_estimators, num_classes=4):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.num_classes = num_classes\n",
    "        self.estimators = []\n",
    "        self.alphas = []\n",
    "        self.weights = None\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        #BEGIN Workspace 3.11\n",
    "        #TODO: Implement Multiclass Adaboost and keep track of the alpha_j\n",
    "\n",
    "        #END Workspace 3.11\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        answer = 0\n",
    "        #BEGIN Workspace 3.12\n",
    "        #TODO: get the labels returned by the global estimator defined as H\n",
    "        #Hint: Use one-hot format to get the labels using the global estimator\n",
    "        #Hint: We don't need predict_proba for this one\n",
    "        \n",
    "        #END Workspace 3.12\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell should run without errors\n",
    "ensemble_handler.fit_model(AdaBoost(10), 'AdaBoost')\n",
    "ensemble_handler.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison**\n",
    "\n",
    "Bonus.3 Add different ensemble methods to the handler (try different parameters), plot, show, and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a handler for ensemble_test, use the created handler for fitting different models.\n",
    "ensemble_handler = EnsembleTest(X_train, y_train, X_test, y_test)\n",
    "decision = get_weak_learner()\n",
    "ensemble_handler.fit_model(decision,'decision_tree')\n",
    "#BEGIN Workspace 3.13.a\n",
    "#TODO Add multiple instances of the ensemble methods. Plot and compare their performance\n",
    "\n",
    "#END Workspace 3.13.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#BEGIN Workspace 3.13.b\n",
    "#TODO Comparison write-up\n",
    "#END Workspace 3.13.b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
